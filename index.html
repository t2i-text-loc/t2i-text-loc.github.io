<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Precise Parameter Localization for Textual Generation in Diffusion Models">
  <meta property="og:title" content="Precise Parameter Localization for Textual Generation in Diffusion Models"/>
  <meta property="og:description" content="Precise Parameter Localization for Textual Generation in Diffusion Models"/>
  <meta property="og:url" content="https://t2i-text-loc.github.io/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/banner.png" />
  <meta property="og:image:width" content="1599"/>
  <meta property="og:image:height" content="600"/>


  <meta name="twitter:title" content="Precise Parameter Localization for Textual Generation in Diffusion Models">
  <meta name="twitter:description" content="Precise Parameter Localization for Textual Generation in Diffusion Models">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/banner.png">
  <meta name="twitter:card" content="Paramter localization for textual generation in diffusion models">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="diffusion models, textual generation, localization, mechanistic interpretability, image editing">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Precise Parameter Localization for Textual Generation in Diffusion Models</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>

  
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Precise Parameter Localization for Textual Generation in Diffusion Models</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block"><strong>ICLR 2025</strong></span>
            </div>

            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=UD4XdGYAAAAJ" target="_blank">Łukasz Staniszewski</a><sup>*1</sup>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?hl=pl&user=OojeQCIAAAAJ" target="_blank">Bartosz Cywiński</a><sup>*1</sup>,</span>
              <span class="author-block">
                <a href="https://franziska-boenisch.de/" target="_blank">Franziska Boenisch</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.pl/citations?hl=en&user=NCGs2HUAAAAJ" target="_blank">Kamil Deja</a><sup>1,3</sup>,
              </span>
              <span class="author-block">
                <a href="https://adam-dziedzic.com/" target="_blank">Adam Dziedzic</a><sup>2</sup>
              </span>
              </div>

              <div class="is-size-5 publication-authors">
                <span class="author-block"><sup>1</sup>Warsaw University of Technology<br> <sup>2</sup>CISPA Helmholtz Center for
                  Information Security<br> <sup>3</sup>IDEAS NCBR</span>
                <span class="eql-cntrb"><small><br><sup>*</sup>Indicates <strong>Equal Contribution</strong>.</small></span>
              </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2502.09935.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->
                  <span class="link-block">
                    <a href="https://arxiv.org/abs/2502.09935" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="#" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (comming soon!)</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->

              <span class="link-block">
                <a href="#BibTeX"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-obp"></i>
                  </span>
                  <span>BibTex</span>
                </a>
              </span> 

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay="" controls="" muted="" loop="" height="100%">
        <!-- Your video here -->
        <source src="static/videos/teaser.mp4" type="video/mp4">
      </video>
      <!-- <img src="static/videos/teaser2.gif" alt="Teaser video" style="width: 100%; height: auto;"> -->
      <h2 class="subtitle has-text-centered">
        <strong>tldr;</strong> Textual content within images is controlled by less than 1% of diffusion models’ parameters, which we localize by activation patching.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Novel diffusion models can synthesize photo-realistic images with integrated high-quality text. Surprisingly, we
            demonstrate through attention activation patching that only less than 1% of diffusion models' parameters, all
            contained in attention layers, influence the generation of textual content within the images.
            Building on this observation, we improve textual generation efficiency and performance by targeting cross and joint
            attention layers of diffusion models.
            We introduce several applications that benefit from localizing the layers responsible for textual content generation.
            We first show that a LoRA-based fine-tuning solely of the localized layers enhances, even more, the general
            text-generation capabilities of large diffusion models while preserving the quality and diversity of the diffusion
            models' generations. Then, we demonstrate how we can use the localized layers to edit textual content in generated
            images. Finally, we extend this idea to the practical use case of preventing the generation of toxic text in a cost-free
            manner.
            In contrast to prior work, our localization approach is broadly applicable across various diffusion model architectures,
            including U-Net (e.g., LDM and SDXL) and transformer-based (e.g., DeepFloyd IF and Stable Diffusion 3), utilizing
            diverse text encoders (e.g., from CLIP to the large language models like T5).
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<section class="hero is-small" style="margin-bottom: 0px;">
  <div class="hero-body" style="margin-bottom: 0px;">
    <div class="container">
      <h2 class="title is-3 has-text-centered">Where do diffusion models generate the content of text?</h2>
      <div class="column is-full">
        <div class="content has-text-justified">
          <p>Building on recent works that show how altering key and value matrices in cross-attention layers can control concept
            generation in diffusion models, we identify which attention layers generate textual content.</p>

          <p><strong>Injection</strong> modifies key and value matrices in cross-attention layers to influence concept generation
            in U-Net-based diffusion models like Stable Diffusion. It simply replaces the text embeddings that are the input to the cross-attention layer with the embeddings of the target prompt. However, it is ineffective for models with joint attention,
            such as SD3 and FLUX, where text embeddings evolve across layers.</p>

          <p><strong>Patching</strong> identifies attention layers by caching and injecting
            activations from a target prompt during another generation. By modifying only text-related activations, patching
            remains effective across different model architectures.</p>
        </div>
        <img src="static/images/img_schema.svg" alt="MY ALT TEXT"
          style="width: 75%; display: block; margin: 0 auto;" />
        <h3 class="subtitle has-text-centered">
          Overview of the localization approaches.
        </h3>
      </div>
      <div class="column is-full">
        <div class="content has-text-justified">
          <p>
            Using our localization approaches, we identified three layers in SDXL (55, 56, and 57), one layer in DeepFloyd IF (17), and one layer in SD3 (10). These layers, when patched, cause the diffusion models to produce the text that closely matches the text in the target prompt.
            Most notably, <strong>this constitutes to less than 1% of the total model parameters</strong>.
          </p>
        </div>
        <img src="static/images/tab.png" alt="MY ALT TEXT" style="width: 50%; display: block; margin: 0 auto 15px;"/>
        <img src="static/images/loc_heatmap.jpg" alt="MY ALT TEXT" style="width: 70%; display: block; margin: 0 auto;"/>
        <h3 class="subtitle has-text-centered">
          Localized attention layers responsible for the content of the generated text.
        </h3>
      </div>
    </div>
  </div>
</section>

<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3 has-text-centered">Localized layers separate textual content from the rest of the image</h2>
      <div class="column is-full">
        <div class="content has-text-justified">
          <p>
            We investigate the information extracted from the prompt by selected layers and its effect on generation. To this end, we use different combinations of templates and keyword texts as the target prompt and measure their impact on the diffusion model’s output. We find that the background consistently aligns with the source prompt, while text alignment varies based on the target prompt. This suggests that the localized layers focus specifically on the text component of the prompt.
          </p>
          <img src="static/images/fig4.png" alt="Figure showing localized layers focusing on text" style="width: 40%; display: block; margin: 0 auto;"/>
        </div>
        <h3 class="subtitle has-text-centered">
          Localized layers focus on the text component of the prompt.
        </h3>
      </div>
    </div>
  </div>
</section>



<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3 has-text-centered">How can we benefit from localized layers?</h2>
      <div class="column is-full">
        <div class="content has-text-justified">
          <h3 class="subtitle has-text-centered">I. Efficient fine-tuning</h3>
          <p>
            We demonstrate that using LoRA-based fine-tuning specifically on the localized layers significantly improves the general
            text-generation abilities of diffusion models. This enhancement is achieved without compromising the quality and
            diversity of outputs produced by the diffusion models. By focusing on localized layers, we only need to fine-tune about
            1% of the model parameters, making our approach both computationally and memory-efficient.
          </p>
          <div style="margin: 20px 0; display: flex; justify-content: center;">
            <div style="width: 43%;">
              <div style="width: 100%; margin-bottom: 20px;">
                <img src="static/images/lora3.jpg" alt="MY ALT TEXT" style="width: 100%; display: block;"/>
              </div>
              <div style="width: 100%;">
                <img src="static/images/clip_embs.jpg" alt="MY ALT TEXT" style="width: 100%; display: block;"/>
              </div>
            </div>
            <div style="width: 50%;">
              <img src="static/images/lora_generations5.jpg" alt="MY ALT TEXT" style="width: 100%; display: block;"/>
            </div>
          </div>
        </div>
        <h3 class="subtitle has-text-centered">
          Fine-tuning only localized layers results in a model that generates images with higher quality and diversity.
        </h3>
        <div class="content has-text-justified">
          <h3 class="subtitle has-text-centered">II. Textual editing</h3>
          <p>
            We can use the localized layers to edit textual content in generated
            images, where the goal is to change the text in the generated image to a new text, while keeping the rest of the image unchanged. Our method works across diverse model archtectures, leveraging different text encoders.
          </p>
          <div class="container" style="display: flex; justify-content: center;">
            <div id="results-carousel" class="carousel results-carousel">
              <div class="item">
                <!-- Your image here -->
                <img src="static/images/sdxl.png" alt="MY ALT TEXT" />
                <h2 class="subtitle has-text-centered">
                  SDXL
                </h2>
              </div>
              <div class="item">
                <!-- Your image here -->
                <img src="static/images/if.png" alt="MY ALT TEXT" />
                <h2 class="subtitle has-text-centered">
                  DeepFloyd IF
                </h2>
              </div>
              <div class="item">
                <!-- Your image here -->
                <img src="static/images/sd3.png" alt="MY ALT TEXT" />
                <h2 class="subtitle has-text-centered">
                  SD3
                </h2>
              </div>
            </div>
          </div>
          Our approach can be directly compared with the Prompt-to-Prompt method where the image edition is controlled only by the text provided by the user.
          <div style="width: 70%; margin: 0 auto;">
            <img src="static/images/tab_edit.png" alt="MY ALT TEXT" style="width: 100%; display: block;"/>
          </div>
        </div>
        <h4 class="subtitle has-text-centered">
          Our method outperforms Prompt-to-Prompt method in text editing task by generating higher-quality images while preserving the rest of the visual content.
        </h4>
        <div class="content has-text-justified">
          <h3 class="subtitle has-text-centered">III. Preventing toxic text generation</h3>
          <p>
            We observe that diffusion models, even the ones equipped with safeguards against generating NSFW (Not Safe For Work) content, tend to simply copy-paste the text from the prompt to the image. As a result, while the visual content may be safe thanks to careful filtering of the fine-tuning dataset, the text in the generated images can still be harmful.
          </p>
          <p>
            We apply our edition technique to prevent the generation of toxic text within images. Our goal is to address scenarios where a model provider exposes a diffusion model for generating images from textual prompts.
          </p>
          <div class="image-container">
            <!-- <div style="width: 49%; margin: 0 auto;"> -->
              <img src="static/images/tab_toxic.png" alt="MY ALT TEXT"/>
            <!-- </div> -->
            <!-- <div style="width: 49%; margin: 0 auto;"> -->
              <img src="static/images/gen_toxic2.png" alt="MY ALT TEXT"/>
            <!-- </div> -->
        </div>
      </div>
      <h4 class="subtitle has-text-centered">
        We remove the toxic text from the final generations with a single pass through the diffusion denoising process without imposing any additional computational cost.
      </h4>
    </div>
  </div>
</section>



<!-- Image carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <img src="static/images/loc_heatmap.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          First image description.
        </h2>
      </div>
      <div class="item">
        <img src="static/images/carousel2.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Second image description.
        </h2>
      </div>
      <div class="item">
        <img src="static/images/carousel3.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
         Third image description.
       </h2>
     </div>
     <div class="item">
      <img src="static/images/carousel4.jpg" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Fourth image description.
      </h2>
    </div>
  </div>
</div>
</div>
</section> -->
<!-- End image carousel -->




<!-- Youtube video -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">

          <div class="publication-video">
            <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End youtube video -->


<!-- Video carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <source src="static/videos/carousel1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
            <source src="static/videos/carousel2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\
            <source src="static/videos/carousel3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End video carousel -->






<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>

      </div>
    </div>
  </section> -->
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
@inproceedings{
  staniszewski2025precise,
  title={Precise Parameter Localization for Textual Generation in Diffusion Models},
  author={{\L}ukasz Staniszewski and Bartosz Cywi{\'n}ski and Franziska Boenisch and Kamil Deja and Adam Dziedzic},
  booktitle={The Thirteenth International Conference on Learning Representations},
  year={2025},
  url={https://openreview.net/forum?id=gdHtZlaaSo}
}
  </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from <a href="https://nerfies.github.io" target="_blank">Nerfies</a>. 

            <br>
            This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->

<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
